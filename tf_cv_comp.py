# -*- coding: utf-8 -*-
"""tf_cv_comp.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dJwwYy8sgs-kJcEWXWk0wb4O-L8Gmozo
"""

import tensorflow as tf

"""creating 0-d tensor"""

tensor_zero_d = tf.constant(4)
print(tensor_zero_d)

"""creating 1-d tensor

"""

tensor_one_d = tf. constant([2,0,3], dtype=tf.float32)
print(tensor_one_d)

"""creating 2-d tensor

"""

tensor_two_d = tf.constant([
    [1,2,3],
    [2,3,4],
    [3,4,5],
    [4,5,6]
])
print(tensor_two_d)

"""creating 3-d sensor"""

tensor_three_d = tf.constant([
    [[1,2,3],
     [1,2,3]],

    [[1,2,3],
     [1,2,3]],

    [[1,2,3],
     [1,2,3]],

    [[1,2,3],
     [1,2,3]]
])

print(tensor_three_d)

import numpy as np

np_array = np.array([1,2,3])
print(np_array)

converted_tensor = tf.convert_to_tensor(np_array)
print(converted_tensor)

eye_tensor = tf.eye(
    num_rows=3,
    num_columns=None,
    batch_shape=None,
    dtype=tf.dtypes.float32,
    name=None
)
print(eye_tensor)

fill_tensor = tf.fill(
    [3,4], 5, name=None, layout=None
)
print(fill_tensor)

tf.ones(
    [2,3],
    dtype=tf.dtypes.float32,
    name=None,
    layout=None
)

tf.shape(
    input, out_type=None, name=None
)

random_tensor = tf.random.uniform(
    [5,],minval=-0.05, maxval=0.05, seed=None
)
print(random_tensor)

"""Building Neural Networks with TensorFlow [Car Price Prediction]

* task
* data
* modeling
* error measurement
* training and optimization
* performance measurement
* validating and testing
* corrective measurements
"""

import tensorflow as tf #models
import pandas as pd #reading and processing data
import seaborn as sns # visualization
from tensorflow.keras.layers import Normalization, Dense, InputLayer
from tensorflow.keras.losses import MeanSquaredError, Huber, MeanAbsoluteError
from tensorflow.keras.optimizers import Adam
import matplotlib.pyplot as plt
from tensorflow.keras.callbacks import History
from tensorflow.keras.metrics import RootMeanSquaredError

"""**Data preparation**"""

data = pd.read_csv("train.csv",",")
data.head()

data.shape

tensor_data = tf.constant(data)
print(tensor_data)

tensor_data = tf.random.shuffle(tensor_data)
print(tensor_data[:5])

X = tensor_data[:,3:-1]
y = tensor_data[:,:-1]
y = tf.expand_dims(y, axis = -1)

normalizer = Normalization()
normalizer.adapt(X)
normalizer(X)

"""**Model Creation and Training**

"""

model = tf.keras.Sequential([
    InputLayer(input_shape = (8,)), #batch
    normalizer, Dense(1),])
model.summary()

tf.keras.utils.plot_model(model,to_file = "model.png", show_shapes=True)

model.compile(optimizer = Adam() ,loss = MeanAbsoluteError(), metrics = RootMeanSquaredError())

"""Huber, when we have an outlier it is going to use mean absolute error, when we have a normal data point we use mean square error"""

history = model.fit(X,y,epochs=100,verbose = 1)

history.history

plt.plot(history.history['loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train'])
plt.show()

plt.plot(history.history['root_mean_squared_error'])
plt.title('model performance')
plt.ylabel('rmse')
plt.xlabel('epoch')
plt.legend(['train'])
plt.show()

model2 = tf.keras.Sequential([
    InputLayer(input_shape = (8,)),
    normalizer,
    Dense(1)
])
model2.summary()

TRAIN_RATIO = 0.8
VAL_RATIO = 0.1
TEST_RATIO = 0.1
DATASET_SIZE = len(X)

X_train = X[:int(DATASET_SIZE*TRAIN_RATIO)]
y_train = y[:int(DATASET_SIZE*TRAIN_RATIO)]
print(X_train.shape)

train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))
train_dataset = train_dataset.shuffle(buffer_size = 8, reshuffle_each_iteration = True).batch(32).prefetch(tf.data.AUTOTUNE)

X_val = X[int(DATASET_SIZE*TRAIN_RATIO):int(DATASET_SIZE*(TRAIN_RATIO+VAL_RATIO))]
y_val = y[int(DATASET_SIZE*TRAIN_RATIO):int(DATASET_SIZE*(TRAIN_RATIO+VAL_RATIO))]
print(X_val.shape)

val_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val))
val_dataset = train_dataset.shuffle(buffer_size = 8, reshuffle_each_iteration = True).batch(32).prefetch(tf.data.AUTOTUNE)

X_test = X[int(DATASET_SIZE*(TRAIN_RATIO+VAL_RATIO)):]
y_test = y[int(DATASET_SIZE*(TRAIN_RATIO+VAL_RATIO)):]
print(X_test.shape)

test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test))
test_dataset = train_dataset.shuffle(buffer_size = 8, reshuffle_each_iteration = True).batch(32).prefetch(tf.data.AUTOTUNE)

normalizer = Normalization()
normalizer.adapt(X_train)
normalizer(X)[:5]

model = tf.keras.Sequential([ InputLayer(input_shape = (8,)),
                              normalizer, Dense(128, activation = "relu"),
                              Dense(128, activation = "relu"),
                               Dense(128, activation = "relu"),
                               Dense(1), ])
model.summary()

tf.keras.utils.plot_model(model, to_file = "model.png", show_shapes=True)

model.compile(optimizer = Adam(learning_rate = 0.1) ,loss = MeanAbsoluteError(), metrics = RootMeanSquaredError())

history = model.fit(train_dataset, validation_data=val_dataset, epochs=100,verbose = 1)

history.history

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train','Val_loss'])
plt.show()

"""**Model Evaluation and Testing**"""

model.evaluate(X_test, y_test)

model.predict(tf.expand_dims(X_test[0],axis = 0))

y_true = list(y_test[:,0].numpy())
y_pred = list(model.predict(X_test)[:,0])
print(y_pred)

ind = np.arange(100)
plt.figure(figsize=(40,20))
width = 0.1
plt.bar(ind, y_pred, width, label='Predicted Car Price')
plt.bar(ind + width, y_true, width, label='Actual Car Price')
plt.xlabel('Actual vs Predicted Prices')
plt.ylabel('Car Price Prices')
plt.show()